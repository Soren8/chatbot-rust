llms:
  - provider_name: "Local Ollama (Free Tier)"
    type: "ollama" 
    base_url: "http://${LOCAL_HOST}:${LOCAL_PORT}"
    model_name: "dolphin3.1-8b"
    context_size: 8192
    tier: "free"
    template: |
      {% if not history %}
        ### System: {{ system_prompt }}
      {% endif %}

      {% if memory_text %}
      ### Memory:
      {{ memory_text.strip() }}
      {% endif %}

      {% for user_input, assistant_response in history %}
      ### User:
      {{ user_input }}

      ### Assistant:
      {{ assistant_response }}
      {% endfor %}
      
      ### User:
      {{ prompt }}
      ### Assistant:

  - provider_name: "Local LMStudio (Free Tier)"
    type: "openai"
    base_url: "http://${LOCAL_HOST}:${LOCAL_PORT}/v1"
    model_name: "local-model"
    tier: "free"

  - provider_name: "GPT-4 (Premium)"
    type: "openai"
    api_key: "${OPENAI_API_KEY}"
    model_name: "gpt-4-turbo"
    tier: "premium"

default_llm: "Local Ollama (Free Tier)"
default_system_prompt: "You are a helpful AI assistant. Provide clear and concise answers to user queries."
session_timeout: 3600
csrf: on # Set to 'off' only for development (localhost over HTTP); this also disables the 'Secure' flag on session cookies.
save_thoughts: true
send_thoughts: false
